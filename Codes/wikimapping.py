import pandas as pd
import re
import bz2
import xml.etree.ElementTree as ET
from tqdm import tqdm

# âœ… ê²½ë¡œ ì„¤ì •
MRCONSO_PATH = "/data/a5252545/capstone/UMLS/MRCONSO.RRF"
MRDEF_PATH = "/data/a5252545/capstone/UMLS/MRDEF.RRF"
REDIRECT_DICT_PATH = "/data/a5252545/capstone/redirect_dict.json"
WIKI_DUMP_PATH = "/data/a5252545/capstone/wikipedia/enwiki-20250401-pages-articles-multistream.xml.bz2"
OUTPUT_PATH = "real_cui_wiki_def_enhanced.csv"

# âœ… 1. Wikipedia title normalization í•¨ìˆ˜
def normalize_title(s):
    if not isinstance(s, str):
        return None
    s = re.sub(r'\(.*?\)', '', s)        # ê´„í˜¸ ì œê±°
    s = re.sub(r'\s+', ' ', s).strip()   # ê³µë°± ì •ë¦¬
    s = re.sub(r'[^\w\s\-]', '', s)    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    s = s.capitalize()                   # ì²«ê¸€ì ëŒ€ë¬¸ì
    return s

# âœ… 2. STR ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
print("ğŸ”¹ Loading MRCONSO...")
conso = pd.read_csv(MRCONSO_PATH, sep='|', header=None, dtype=str, usecols=[0, 11, 14])
conso.columns = ['CUI', 'SAB', 'STR']
conso = conso.dropna(subset=['STR'])
conso = conso[conso['SAB'].isin(['MSH', 'SNOMEDCT_US', 'MEDLINE'])].drop_duplicates(subset=['CUI', 'STR'])
conso['norm_title'] = conso['STR'].apply(normalize_title)

# âœ… 3. Redirect ì‚¬ì „ ë¶ˆëŸ¬ì˜¤ê¸°
import json
print("ğŸ”¹ Loading Redirect dict...")
with open(REDIRECT_DICT_PATH, 'r') as f:
    redirect_dict = json.load(f)

# âœ… 4. UMLS ì •ì˜ ë¡œë“œ
def_df = pd.read_csv(MRDEF_PATH, sep='|', header=None, usecols=[0, 5], names=['CUI', 'DEF'], dtype=str)
mrdef_map = def_df.groupby('CUI')['DEF'].first().to_dict()

# âœ… 5. Wikipedia ì •ì˜ ë§¤í•‘ í•¨ìˆ˜
def parse_wikipedia_dump(dump_path, titles_set):
    wiki_defs = {}
    print("ğŸ”¹ Parsing Wikipedia XML dump...")
    with bz2.open(dump_path, 'rt', encoding='utf-8') as f:
        context = ET.iterparse(f, events=('end',))
        for event, elem in tqdm(context, desc="Parsing", unit=" pages"):
            if elem.tag.endswith('page'):
                title = elem.findtext('./title')
                text = elem.findtext('./revision/text')
                if title in titles_set and text:
                    first_sentence = extract_first_sentence(text)
                    if first_sentence:
                        wiki_defs[title] = first_sentence
                elem.clear()
    return wiki_defs

def extract_first_sentence(text):
    candidates = re.split(r'\.(\s|\n)', text)
    for sent in candidates:
        sent = sent.strip()
        if len(sent) > 20:
            return sent + '.'
    return None

# âœ… 6. ì „ì²´ íƒ€ì´í‹€ ì…‹ ë§Œë“¤ê¸° (ì •ê·œí™” + redirect í¬í•¨)
norm_titles = set(conso['norm_title'].dropna())
redirected_titles = {redirect_dict.get(t, t) for t in norm_titles}
titles_to_match = norm_titles.union(redirected_titles)

# âœ… 7. Wikipedia í…ìŠ¤íŠ¸ ì •ì˜ ë§¤í•‘ ìˆ˜í–‰
wiki_def_map = parse_wikipedia_dump(WIKI_DUMP_PATH, titles_to_match)

# âœ… 8. termì— ëŒ€í•´ ìœ„í‚¤ ì •ì˜ or fallback ì •ì˜ ì‚½ì…
def get_best_definition(cui, raw_str, norm_title):
    # ìš°ì„ ìˆœìœ„ 1: redirect â†’ wiki
    redirected = redirect_dict.get(norm_title, norm_title)
    if redirected in wiki_def_map:
        return wiki_def_map[redirected], 'wiki'
    # ìš°ì„ ìˆœìœ„ 2: wiki ì§ë§¤í•‘
    if norm_title in wiki_def_map:
        return wiki_def_map[norm_title], 'wiki'
    # ìš°ì„ ìˆœìœ„ 3: UMLS ì •ì˜
    if cui in mrdef_map:
        return mrdef_map[cui], 'umls'
    return None, 'none'

# âœ… 9. ìµœì¢… ì •ì˜ í…Œì´ë¸” ìƒì„±
records = []
for _, row in tqdm(conso.iterrows(), total=len(conso)):
    cui, s, norm = row['CUI'], row['STR'], row['norm_title']
    definition, source = get_best_definition(cui, s, norm)
    if definition:
        records.append({
            'CUI': cui,
            'STR': s,
            'definition': definition,
            'source': source
        })

# âœ… 10. ì €ì¥
df_out = pd.DataFrame(records)
df_out.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')
print(f"âœ… ì •ì˜ ë§¤í•‘ ì™„ë£Œ â†’ ì´ {len(df_out)}ê°œ term ì €ì¥ â†’ {OUTPUT_PATH}")